{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " # 67-212 Homework 4\n",
    "## Summarization of Customer Reviews\n",
    "\n",
    "In this homework, you will work on the customer reviews of products available at online marketplaces. Managing and mining these reviews can be a tricky and cumbersome task. Let's say you are working as an NLP expert for an online marketplace such as an apparel company that accepts customer reviews. However, there are too many reviews to handle and some of them are very long. So it is difficult to go through all of them. Your task is to build an automatic title (summary) generator for customer reviews. Below is a sample of reviews and their corresponding titles.\n",
    "\n",
    "<center><img src='example2.png' width='300' height='150'></center>\n",
    "\n",
    "\n",
    "In this notebook, you will build a deep neural network that functions as part of an end-to-end text summarization pipeline, using the Python version of the [OpenNMT toolkit]. Your completed system will accept a given product review as input and outputs a title summarizing it.\n",
    "\n",
    "This homework is organized into 4 main parts:\n",
    "\n",
    "1. **Preprocess** - You'll clean your text and split it into training, development, and testing sets.\n",
    "2. **Modeling** Use OpenNMT to create a model that accepts a review (sequence of words) as input and returns a title summarizing it.\n",
    "3. **Prediction** Run the model on the Test set.\n",
    "4. **Evaluation** Evaluate the quality of the system using the ROUGE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper method for loading data from files\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load dataset\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    f=open(input_file, \"r\")\n",
    "    return f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We begin by investigating the dataset that will be used to train and evaluate your pipeline.  We'll be using an e-Commerce clothing reviews dataset. The dataset has over 23,000 customer reviews. And apart from the `reviews`, the other key features are the `titles` and the `ratings` assigned to each review by the customers.\n",
    "\n",
    "### Load Data\n",
    "The data is located in `data/women_clothing_ecommerce_reviews` and `data/women_clothing_ecommerce_review_titles`. The `reviews` file contains the full reviews with their corresponding titles in the `review_titles` file. Load the reviews and their titles these files from running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "# Load Reviews data\n",
    "reviews = load_data('data/women_clothing_ecommerce_reviews.txt')\n",
    "# Load Titles data\n",
    "titles = load_data('data/women_clothing_ecommerce_review_title.txt')\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files\n",
    "Each line in `reviews` contains a review with the respective translation in each line of `titles`. Let's view the first three lines from each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TITLE Line 1:  Some major design flaws\n\nREVIEW Line 1:  I had such high hopes for this dress and really wanted it to work for me. i initially ordered the petite small (my usual size) but i found this to be outrageously small. so small in fact that i could not zip it up! i reordered it in petite medium, which was just ok. overall, the top half was comfortable and fit nicely, but the bottom half had a very tight under layer and several somewhat cheap (net) over layers. imo, a major design flaw was the net over layer sewn directly into the zipper - it c\n\n****************************\nTITLE Line 2:  My favorite buy!\n\nREVIEW Line 2:  I love, love, love this jumpsuit. it's fun, flirty, and fabulous! every time i wear it, i get nothing but great compliments!\n\n****************************\nTITLE Line 3:  Flattering shirt\n\nREVIEW Line 3:  This shirt is very flattering to all due to the adjustable front tie. it is the perfect length to wear with leggings and it is sleeveless so it pairs well with any cardigan. love this shirt!!!\n\n****************************\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(3):\n",
    "        print('TITLE Line {}:  {}'.format(sample_i + 1, titles[sample_i]))\n",
    "        print('REVIEW Line {}:  {}'.format(sample_i + 1, reviews[sample_i]))\n",
    "        print(\"****************************\")\n"
   ]
  },
  {
   "source": [
    "# 1. Preprocess (10 points)\n",
    "Before getting started with text summarization, let's preprocess the text in Title and Review Text. The objective is to make the text suitable for modeling by taking off as much noise as possible."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary of all possible contractions and their expanded forms\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Clean (4 points)\n",
    "\n",
    "Define a function `clean(text)` that will preprocess and clean the text `text`.\n",
    "\n",
    "We would like to carry out the following preprocessing operations:\n",
    "\n",
    "- Convert text to lowercase\n",
    "- Expand the contractions (\"isn't\" to \"is not\"), a dictionary of all possible contractions and their expanded forms (`contraction_mapping`) is provided for you.\n",
    "- Remove everything from the text except alphabets, '.' and ','\n",
    "- Remove single-character tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'hello you do not like this. too sxasxas much complex so much wkqda'"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "def clean(text):\n",
    "    \"\"\"\n",
    "    clean x\n",
    "    :param x: text to be cleaned\n",
    "    :return: cleaned text as per the guidelines specified above\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "\n",
    "    return \"\"\n",
    "\n"
   ]
  },
  {
   "source": [
    "Now preprocess the text in the lists __reviews__ and __titles__."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess review text\n",
    "cleaned_reviews = [clean(r) for r in reviews]\n",
    "cleaned_titles = [clean(t) for t in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('my favorite buy',\n",
       " 'love, love, love this jumpsuit. it is fun, flirty, and fabulous every time wear it, get nothing but great compliments')"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "cleaned_titles[1], cleaned_reviews[1]\n",
    "# this should return: \n",
    "# ('my favorite buy','love, love, love this jumpsuit. it is fun, flirty, and fabulous every time wear it, get nothing but great compliments')"
   ]
  },
  {
   "source": [
    "## 1.2. Explore the Vocabulary  (6 points)\n",
    "The complexity of the problem is determined by the complexity of the vocabulary.  A more complex vocabulary is a more complex problem.  Let's look at the complexity of the dataset we'll be working with.\n",
    "- Calculate the total number of words in the reviews\n",
    "- Calculate the number of unique words in the reviews\n",
    "- Calculate the total number of words in the titles\n",
    "- Calculate the number of unique words in the titles\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # TODO: Implement"
   ]
  },
  {
   "source": [
    "Your code should print the following:\n",
    "```python\n",
    "1125528 Review words.\n",
    "22628 unique Review words.\n",
    "10 Most common words in the Reviews dataset:\n",
    "\"the\" \"and\" \"it\" \"is\" \"this\" \"to\" \"in\" \"not\" \"but\" \"on\"\n",
    "\n",
    "64407 Title words.\n",
    "4575 unique Title words.\n",
    "10 Most common words in the Title dataset:\n",
    "\"and\" \"great\" \"love\" \"dress\" \"but\" \"cute\" \"beautiful\" \"not\" \"for\" \"top\"\n",
    "```\n",
    "``"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 2. Train, validation, Test splitting (10 points)\n",
    "\n",
    "You will have to **manually** split the reviews dataset into training, validation (development), and testing sets. \n",
    "10% of the data for validation, 10% for testing and 80% for training the model. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement"
   ]
  },
  {
   "source": [
    "# 3. Build a model using OpenNMT (60 points)\n",
    "\n",
    "[OpenNMT](https://opennmt.net/) is an open source ecosystem for neural machine translation and neural sequence learning. Started in December 2016 by the [Harvard NLP group](https://nlp.seas.harvard.edu/) and [SYSTRAN](https://translate.systran.net/), the project has since been used in several research and industry applications. It is currently maintained by SYSTRAN and Ubiqus.\n",
    "\n",
    "OpenNMT provides implementations in two popular deep learning frameworks: [OpenNMT-py](https://opennmt.net/OpenNMT-py/), including a python-based implementation of the encoder-decoder architecture, and [OpenNMT-tf](https://opennmt.net/OpenNMT-tf), an implementation based on tensorflow.\n",
    "\n",
    "In this homework you will be using OpenNMT, get started with it and its component in order to user for training the title generation system, and testing it. \n",
    "\n",
    "Take a look at the OpenNMT-py documentation and its [quickstart](https://opennmt.net/OpenNMT-py/quickstart.html) page to familiarize yourself with the main training workflow within OpenNMT.\n",
    "\n",
    "You will be evaluating the quality of the system using the ROUGE de-facto summarization metric. It compares the output of your system with the gold summaries and generates a score. \n",
    "You can use this [ROUGE python implementation](https://pypi.org/project/rouge/).\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Preprocess (10 points)\n",
    "# TODO: Implement"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 74,
   "outputs": []
  },
  {
   "source": [
    "# Train (25 points)\n",
    "# TODO: Implement"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Test (25 points)\n",
    "# TODO: Implement"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 75,
   "outputs": []
  },
  {
   "source": [
    "# 4. Evaluate the model (20 points)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Evaluate\n",
    "# TODO: Implement"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}